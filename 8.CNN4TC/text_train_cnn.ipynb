{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"text_train_cnn.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1pfdIPoYTaMd2JpBBVVBNUhrnNG5fslDe","authorship_tag":"ABX9TyNou5ah75HqUwiMrPZ2MxcB"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"pCMFO7AcnCpi"},"source":["# !pip3 uninstall -y tensorflow\n","# !pip3 install tensorflow==1.15"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WVo4ZFTnr4jp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605865897102,"user_tz":-540,"elapsed":1034,"user":{"displayName":"‍이노아[ 학부재학 / 통계학과 ]","photoUrl":"","userId":"12005920549152507828"}},"outputId":"d67e04b3-5a9b-458c-be15-69966e8b5389"},"source":["cd drive/My Drive/cose474/assignment8/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/cose474/assignment8\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5oDrRoKNn1Kz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605865899244,"user_tz":-540,"elapsed":3166,"user":{"displayName":"‍이노아[ 학부재학 / 통계학과 ]","photoUrl":"","userId":"12005920549152507828"}},"outputId":"a8813c6c-af7d-475a-9e71-63ba8f9ab72c"},"source":["import tensorflow as tf\n","print('TF version:',tf.__version__)\n","import tensorflow as tf\n","import numpy as np\n","import os\n","import time\n","import datetime\n","import re\n","import smart_open\n","import pickle\n","import data_helpers as dh\n","from gensim.models.keyedvectors import KeyedVectors\n","from text_cnn import TextCNN"],"execution_count":null,"outputs":[{"output_type":"stream","text":["TF version: 1.15.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wNihTTwZnCJv"},"source":["# Parameters\n","# ==================================================\n","\n","# Data loading params\n","tf.flags.DEFINE_float(\"dev_sample_percentage\", .1, \"Percentage of the training data to use for validation\")\n","tf.flags.DEFINE_string(\"trec_train_file\", \"./data/TREC/traindata.txt\", \"Data source for the training\")\n","tf.flags.DEFINE_string(\"mr_train_file_pos\", \"./data/MR/rt-polarity.pos\", \"Data source for the MR training\")\n","tf.flags.DEFINE_string(\"mr_train_file_neg\", \"./data/MR/rt-polarity.neg\", \"Data source for the MR training\")\n","\n","\n","# rand: None, else:'./data/GoogleNews-vectors-negative300.bin.gz\"\n","tf.flags.DEFINE_string(\"word2vec\", \"./data/GoogleNews-vectors-negative300.bin.gz\", \"Word2vec file with pre-trained embeddings (default: None)\")\n","tf.flags.DEFINE_string(\"task\", \"TREC\", \"Choose the classification task\")\n","\n","#초모수는 \n","# Model Hyperparameters\n","tf.flags.DEFINE_integer(\"vocab_size\", 30000, \"Vocabulary size (defualt: 0)\")\n","tf.flags.DEFINE_integer(\"num_classes\", 0, \"The number of labels (defualt: 0)\")\n","tf.flags.DEFINE_integer(\"max_length\", 0, \"max sequence length (defualt: 0)\")\n","tf.flags.DEFINE_integer(\"embedding_dim\", 300, \"Dimensionality of character embedding (default: 128)\")\n","tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n","tf.flags.DEFINE_integer(\"num_filters\", 100, \"Number of filters per filter size (default: 128)\")\n","tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n","tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0001, \"L2 regularization lambda (default: 0.0)\")\n","tf.flags.DEFINE_float(\"lr_decay\", 0.99, \"Learning rate decay rate (default: 0.98)\")\n","tf.flags.DEFINE_float(\"lr\", 0.1, \"Learning rate(default: 0.01)\")\n","\n","# Training parameters\n","tf.flags.DEFINE_integer(\"batch_size\", 50, \"Batch Size (default: 64)\")\n","tf.flags.DEFINE_integer(\"num_epochs\", 200, \"Number of training epochs (default: 200)\")\n","tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n","tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n","tf.flags.DEFINE_integer(\"num_checkpoints\", 3, \"Number of checkpoints to store (default: 5)\")\n","# Misc Parameters\n","tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n","tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n","\n","FLAGS = tf.flags.FLAGS\n","\n","def preprocess():\n","    # Load data\n","    print(\"Loading data...\")\n","    if FLAGS.task == \"MR\":\n","        x_text, y = dh.load_mr_data(FLAGS.mr_train_file_pos, FLAGS.mr_train_file_neg)\n","    elif FLAGS.task == \"TREC\":\n","        x_text, y = dh.load_trec_data(FLAGS.trec_train_file)\n","        pass # TREC data 전처리 구현\n","\n","    # Build vocabulary\n","    word_id_dict, _ = dh.buildVocab(x_text, FLAGS.vocab_size) # training corpus를 토대로 단어사전 구축\n","    FLAGS.vocab_size = len(word_id_dict) + 4 #30000 + 4\n","    print(\"vocabulary size: \", FLAGS.vocab_size)\n","\n","    for word in word_id_dict.keys():\n","        word_id_dict[word] += 4  # <pad>: 0, <unk>: 1, <s>: 2 (a: 0 -> 4)\n","    word_id_dict['<pad>'] = 0 # zero padding을 위한 토큰\n","    word_id_dict['<unk>'] = 1 # OOV word를 위한 토큰\n","    word_id_dict['<s>'] = 2 # 문장 시작을 알리는 start 토큰\n","    word_id_dict['</s>'] = 3 # 문장 마침을 알리는 end 토큰\n","\n","    x = dh.text_to_index(x_text, word_id_dict, max(list(map(int, FLAGS.filter_sizes.split(\",\")))) - 1) # i am a boy, word_id_dict, max([3,4,5]) -> 5 - 1\n","    x, FLAGS.max_length = dh.train_tensor(x) # 문장 max length를 바탕으로 batch 구성\n","\n","    # Randomly shuffle data\n","    np.random.seed(10)\n","    shuffle_indices = np.random.permutation(np.arange(len(y)))\n","    x_shuffled = x[shuffle_indices]\n","    y_shuffled = y[shuffle_indices]\n","\n","    # Split train/dev set\n","    dev_sample_index = -1 * int(FLAGS.dev_sample_percentage * float(len(y)))\n","    x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n","    y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n","\n","    FLAGS.num_classes = y_train.shape[1] # 2 (eg., [0, 1]), class 개수를 y shape로 부터 획득\n","\n","    del x, x_text, y, x_shuffled, y_shuffled\n","    print(x_train)\n","    print(y_train)\n","\n","    print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))\n","    return x_train, y_train, word_id_dict, x_dev, y_dev\n","\n","def train(x_train, y_train, word_id_dict, x_dev, y_dev):\n","    # Training\n","    # ==================================================\n","\n","    with tf.Graph().as_default():\n","        session_conf = tf.ConfigProto(\n","          allow_soft_placement=FLAGS.allow_soft_placement,\n","          log_device_placement=FLAGS.log_device_placement)\n","        sess = tf.Session(config=session_conf)\n","        with sess.as_default():\n","            cnn = TextCNN(FLAGS.flag_values_dict())\n","\n","            # Define Training procedure\n","            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n","            # lr decay\n","            decayed_lr = tf.train.exponential_decay(FLAGS.lr, global_step, 1000, FLAGS.lr_decay, staircase=True)\n","            optimizer = tf.train.AdadeltaOptimizer(decayed_lr)\n","            grads_and_vars = optimizer.compute_gradients(cnn.loss)\n","            train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n","\n","            # Keep track of gradient values and sparsity (optional)\n","            grad_summaries = []\n","            for g, v in grads_and_vars:\n","                if g is not None:\n","                    grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n","                    sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n","                    grad_summaries.append(grad_hist_summary)\n","                    grad_summaries.append(sparsity_summary)\n","            grad_summaries_merged = tf.summary.merge(grad_summaries)\n","\n","            # Output directory for models and summaries\n","            timestamp = str(int(time.time()))\n","            out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n","            print(\"Writing to {}\\n\".format(out_dir))\n","\n","            # Summaries for loss and accuracy\n","            loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n","            acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n","\n","            # Train Summaries\n","            train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n","            train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n","            train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n","\n","            # Dev summaries\n","            dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n","            dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n","            dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n","\n","            # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n","            checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n","            checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n","            if not os.path.exists(checkpoint_dir):\n","                os.makedirs(checkpoint_dir)\n","            saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n","\n","            # Write vocabulary, configuration\n","            with smart_open.smart_open(os.path.join(out_dir, \"vocab\"), 'wb') as f:\n","                pickle.dump(word_id_dict, f)\n","            with smart_open.smart_open(os.path.join(out_dir, \"config\"), 'wb') as f:\n","                pickle.dump(FLAGS.flag_values_dict(), f)\n","\n","            # Initialize all variables\n","            sess.run(tf.global_variables_initializer())\n","\n","            if FLAGS.word2vec: # word2vec 활용 시\n","                print(\"Loading W2V data...\")\n","                pre_emb = KeyedVectors.load_word2vec_format(FLAGS.word2vec, binary=True) #pre-trained word2vec load\n","                pre_emb.init_sims(replace=True)\n","                num_keys = len(pre_emb.vocab)\n","                print(\"loaded word2vec len \", num_keys)\n","\n","                # initial matrix with random uniform, pretrained word2vec으로 vocabulary 내 단어들을 초기화하기 위핸 weight matrix 초기화\n","                initW = np.random.uniform(-0.25, 0.25, (FLAGS.vocab_size, FLAGS.embedding_dim))\n","                # load any vectors from the word2vec\n","                print(\"init initW cnn.W in FLAG\")\n","                for w in word_id_dict.keys():\n","                    arr = []\n","                    s = re.sub('[^0-9a-zA-Z]+', '', w)\n","                    if w in pre_emb: # 직접 구축한 vocab 내 단어가 google word2vec에 존재하면\n","                        arr = pre_emb[w] # word2vec vector를 가져옴\n","                    elif w.lower() in pre_emb: # 소문자로도 확인\n","                        arr = pre_emb[w.lower()]\n","                    elif s in pre_emb: # 전처리 후 확인\n","                        arr = pre_emb[s]\n","                    elif s.isdigit(): # 숫자이면\n","                        arr = pre_emb['1']\n","                    if len(arr) > 0: # 직접 구축한 vocab 내 단어가 google word2vec에 존재하면\n","                        idx = word_id_dict[w] # 단어 index\n","                        initW[idx] = np.asarray(arr).astype(np.float32) # 적절한 index에 word2vec word 할당\n","                print(\"assigning initW to cnn. len=\" + str(len(initW)))\n","                sess.run(cnn.W.assign(initW)) # initW를 cnn.W에 할당\n","\n","            def train_step(x_batch, y_batch):\n","                feed_dict = {\n","                  cnn.input_x: x_batch,\n","                  cnn.input_y: y_batch,\n","                  cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n","                }\n","                _, step, lr, summaries, loss, accuracy = sess.run(\n","                    [train_op, global_step, decayed_lr, train_summary_op, cnn.loss, cnn.accuracy],\n","                    feed_dict)\n","                time_str = datetime.datetime.now().isoformat()\n","                print(\"{}: step {}, loss {:g}, lr{:g}, acc {:g}\".format(time_str, step, loss, lr, accuracy))\n","                train_summary_writer.add_summary(summaries, step)\n","\n","            def dev_step(x_batch, y_batch, writer=None):\n","                feed_dict = {\n","                  cnn.input_x: x_batch,\n","                  cnn.input_y: y_batch,\n","                  cnn.dropout_keep_prob: 1.0\n","                }\n","                step, summaries, loss, accuracy = sess.run(\n","                    [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n","                    feed_dict)\n","                time_str = datetime.datetime.now().isoformat()\n","                print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n","                if writer:\n","                    writer.add_summary(summaries, step)\n","                return accuracy\n","\n","            # Generate batches\n","            batches = dh.batch_iter(list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs)\n","            # Training loop. For each batch...\n","            max = 0\n","            for batch in batches:\n","                x_batch, y_batch = zip(*batch)\n","                train_step(x_batch, y_batch)\n","                current_step = tf.train.global_step(sess, global_step)\n","                if current_step % FLAGS.evaluate_every == 0:\n","                    print(\"\\nEvaluation:\")\n","                    accuracy = dev_step(x_dev, y_dev, writer=dev_summary_writer)\n","                    print(\"\")\n","                    if accuracy > max:\n","                        max = accuracy\n","                        path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n","                        print(\"Saved model checkpoint to {}\\n\".format(path))\n","\n","def main(argv=None):\n","    x_train, y_train, word_id_dict, x_dev, y_dev = preprocess()\n","    train(x_train, y_train, word_id_dict, x_dev, y_dev)\n","\n","if __name__ == '__main__':\n","    tf.app.run()"],"execution_count":null,"outputs":[]}]}